{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA Task Agent (Self-Contained)\n",
    "\n",
    "**For Colab GPU Extension - All code in notebook**\n",
    "\n",
    "1. Select Kernel → Colab → T4 GPU\n",
    "2. Run cells sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:23:04.032326Z",
     "iopub.status.busy": "2026-01-19T09:23:04.032049Z",
     "iopub.status.idle": "2026-01-19T09:23:13.237735Z",
     "shell.execute_reply": "2026-01-19T09:23:13.236844Z",
     "shell.execute_reply.started": "2026-01-19T09:23:04.032301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hMon Jan 19 09:23:13 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers peft bitsandbytes accelerate datasets huggingface-hub\n",
    "\n",
    "# Verify GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Config File\n",
    "\n",
    "Upload `configs/agent_config.json` when prompted, or run the cell below to create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:23:28.839649Z",
     "iopub.status.busy": "2026-01-19T09:23:28.838941Z",
     "iopub.status.idle": "2026-01-19T09:23:28.845889Z",
     "shell.execute_reply": "2026-01-19T09:23:28.845356Z",
     "shell.execute_reply.started": "2026-01-19T09:23:28.839616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config created\n"
     ]
    }
   ],
   "source": [
    "# Create config\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"base_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"adapter_path\": \"models/lora-adapter\",\n",
    "        \"max_length\": 512\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('configs/agent_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"✓ Config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:23:38.768311Z",
     "iopub.status.busy": "2026-01-19T09:23:38.767718Z",
     "iopub.status.idle": "2026-01-19T09:23:38.779838Z",
     "shell.execute_reply": "2026-01-19T09:23:38.779083Z",
     "shell.execute_reply.started": "2026-01-19T09:23:38.768284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 360 samples\n",
      "  Train: 324, Eval: 36\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation code (inline)\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "random.seed(42)\n",
    "\n",
    "def generate_date(days_offset=0):\n",
    "    return (datetime.now() + timedelta(days=days_offset)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Generate samples\n",
    "samples = []\n",
    "\n",
    "# Tool invocations (add_task)\n",
    "tasks = [\"buy groceries\", \"finish report\", \"call dentist\", \"gym workout\", \"read book\"]\n",
    "for _ in range(120):\n",
    "    task = random.choice(tasks)\n",
    "    days = random.choice([0, 1, 3, 7])\n",
    "    samples.append({\n",
    "        \"instruction\": f\"Add a task to {task}\",\n",
    "        \"analysis\": f\"User wants to create a new task '{task}'.\",\n",
    "        \"action\": f'add_task(title=\"{task.capitalize()}\", due_date=\"{generate_date(days)}\")'\n",
    "    })\n",
    "\n",
    "# List tasks\n",
    "for _ in range(60):\n",
    "    samples.append({\n",
    "        \"instruction\": random.choice([\"What tasks do I have?\", \"Show my tasks\", \"List tasks\"]),\n",
    "        \"analysis\": \"User is requesting a list of all tasks.\",\n",
    "        \"action\": \"list_tasks()\"\n",
    "    })\n",
    "\n",
    "# Summarize tasks\n",
    "for _ in range(60):\n",
    "    samples.append({\n",
    "        \"instruction\": random.choice([\"Summarize my tasks\", \"Task overview\", \"How many tasks?\"]),\n",
    "        \"analysis\": \"User wants a summary of their tasks.\",\n",
    "        \"action\": \"summarize_tasks()\"\n",
    "    })\n",
    "\n",
    "# Direct responses\n",
    "convs = [\n",
    "    (\"Hello\", \"Hello! How can I help you manage your tasks today?\"),\n",
    "    (\"Hi\", \"Hi! I'm here to help you with your tasks.\"),\n",
    "    (\"Thanks\", \"You're welcome! Let me know if you need anything else.\")\n",
    "]\n",
    "for instruction, response in convs * 40:\n",
    "    samples.append({\"instruction\": instruction, \"final\": response})\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(samples)\n",
    "split_idx = int(len(samples) * 0.9)\n",
    "\n",
    "with open('data/train.json', 'w') as f:\n",
    "    json.dump(samples[:split_idx], f)\n",
    "    \n",
    "with open('data/eval.json', 'w') as f:\n",
    "    json.dump(samples[split_idx:], f)\n",
    "\n",
    "print(f\"✓ Generated {len(samples)} samples\")\n",
    "print(f\"  Train: {split_idx}, Eval: {len(samples) - split_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:23:56.847710Z",
     "iopub.status.busy": "2026-01-19T09:23:56.847412Z",
     "iopub.status.idle": "2026-01-19T09:23:57.251854Z",
     "shell.execute_reply": "2026-01-19T09:23:57.250977Z",
     "shell.execute_reply.started": "2026-01-19T09:23:56.847685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c670f63f844be795e62302cb91f963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login (get token from https://huggingface.co/settings/tokens)\n",
    "# Accept license: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:25:16.630268Z",
     "iopub.status.busy": "2026-01-19T09:25:16.629923Z",
     "iopub.status.idle": "2026-01-19T09:25:16.700188Z",
     "shell.execute_reply": "2026-01-19T09:25:16.699643Z",
     "shell.execute_reply.started": "2026-01-19T09:25:16.630234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '67d9cbbb7a087207dfdaa2cd',\n",
       " 'name': 'altruvi',\n",
       " 'fullname': 'Aswani Sahoo',\n",
       " 'email': 'aswanisahoo227@gmail.com',\n",
       " 'emailVerified': True,\n",
       " 'canPay': False,\n",
       " 'billingMode': 'prepaid',\n",
       " 'periodEnd': 1769904000,\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/6637de6fb2c275008f01f7b64de54319.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'taskai',\n",
       "   'role': 'read',\n",
       "   'createdAt': '2026-01-19T06:40:18.061Z'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "whoami()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning (1-2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:25:54.259931Z",
     "iopub.status.busy": "2026-01-19T09:25:54.259618Z",
     "iopub.status.idle": "2026-01-19T09:29:43.969350Z",
     "shell.execute_reply": "2026-01-19T09:29:43.968634Z",
     "shell.execute_reply.started": "2026-01-19T09:25:54.259905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 09:26:19.291957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768814779.764393      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768814779.895483      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768814780.990256      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768814780.990291      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768814780.990294      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768814780.990296      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffad726f4f91438980a223dda03cddb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4729010dbc2f40ed975ccc44903a5ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1773a5d998894ab7a69f40f01b6437f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befdb1fb409e4c1082000ad43538d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d738b8fb59f5421d818a178122885186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bee97265a04fdb9304167418dae3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2e6a12ae264bd39a489ed0675e0119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f953a3ff198a469ab913cfaa698b6f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d458eadb77cd4e8eadcd3bb52a2d6732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4019c907e4356b9d64c2256864416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a065e9f030914b6cab7ae294b14faf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b8f307f36146f68d290581353348cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with LoRA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load config\n",
    "with open('configs/agent_config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "model_name = config[\"model\"][\"base_model\"]\n",
    "print(f\"Model: {model_name}\")\n",
    "\n",
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=config[\"training\"][\"lora_r\"],\n",
    "    lora_alpha=config[\"training\"][\"lora_alpha\"],\n",
    "    target_modules=config[\"training\"][\"target_modules\"],\n",
    "    lora_dropout=config[\"training\"][\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"✓ Model loaded with LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:31:07.930691Z",
     "iopub.status.busy": "2026-01-19T09:31:07.929884Z",
     "iopub.status.idle": "2026-01-19T09:31:08.840948Z",
     "shell.execute_reply": "2026-01-19T09:31:08.840224Z",
     "shell.execute_reply.started": "2026-01-19T09:31:07.930659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d8568ebfe2493582b8b6eee9b95c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd6c63621634a368e4d15b75eaac9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6627d71a9ec046c99f8196c441504d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbd1edbc6694854af985d61feaddbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared 324 train, 36 eval samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset - UPDATED VERSION\n",
    "def format_sample(sample):\n",
    "    if \"action\" in sample:\n",
    "        response = f\"<analysis>\\n{sample['analysis']}\\n</analysis>\\n\\n<action>\\n{sample['action']}\\n</action>\"\n",
    "    else:\n",
    "        response = f\"<final>\\n{sample['final']}\\n</final>\"\n",
    "    return {\"instruction\": sample[\"instruction\"], \"response\": response}\n",
    "\n",
    "def prepare_dataset(path):\n",
    "    with open(path) as f:\n",
    "        data = [format_sample(s) for s in json.load(f)]\n",
    "    \n",
    "    dataset = Dataset.from_list(data)\n",
    "    \n",
    "    def apply_template(ex):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a task assistant. Use <analysis> and <action> for tools, <final> for conversation.\"},\n",
    "            {\"role\": \"user\", \"content\": ex[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": ex[\"response\"]}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = dataset.map(apply_template, remove_columns=[\"instruction\", \"response\"])\n",
    "    \n",
    "    def tokenize(ex):\n",
    "        # Add padding=True and return_tensors removed\n",
    "        tokenized = tokenizer(\n",
    "            ex[\"text\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\",  # <--- Changed: pad to max_length\n",
    "            return_tensors=None  # <--- Changed: don't convert to tensors yet\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    return dataset.map(tokenize, remove_columns=[\"text\"])\n",
    "\n",
    "train_dataset = prepare_dataset('data/train.json')\n",
    "eval_dataset = prepare_dataset('data/eval.json')\n",
    "print(f\"✓ Prepared {len(train_dataset)} train, {len(eval_dataset)} eval samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T09:31:32.226648Z",
     "iopub.status.busy": "2026-01-19T09:31:32.226363Z",
     "iopub.status.idle": "2026-01-19T10:00:25.423514Z",
     "shell.execute_reply": "2026-01-19T10:00:25.422769Z",
     "shell.execute_reply.started": "2026-01-19T09:31:32.226626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 28:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.052637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.022275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete! Model saved to models/lora-adapter\n"
     ]
    }
   ],
   "source": [
    "# Training - UPDATED VERSION\n",
    "from transformers import default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/lora-adapter\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator  # <--- Use default collator for pre-padded data\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"models/lora-adapter\")\n",
    "tokenizer.save_pretrained(\"models/lora-adapter\")\n",
    "print(\"\\n✓ Training complete! Model saved to models/lora-adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Trained Model\n",
    "\n",
    "Download to use locally (skip if using Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T10:04:21.979931Z",
     "iopub.status.busy": "2026-01-19T10:04:21.979575Z",
     "iopub.status.idle": "2026-01-19T10:04:38.860122Z",
     "shell.execute_reply": "2026-01-19T10:04:38.859066Z",
     "shell.execute_reply.started": "2026-01-19T10:04:21.979900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: models/lora-adapter/ (stored 0%)\n",
      "  adding: models/lora-adapter/tokenizer.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 85%)\n",
      "  adding: models/lora-adapter/checkpoint-123/ (stored 0%)\n",
      "  adding: models/lora-adapter/checkpoint-123/rng_state.pth (deflated 26%)\n",
      "  adding: models/lora-adapter/checkpoint-123/training_args.bin (deflated 53%)\n",
      "  adding: models/lora-adapter/checkpoint-123/README.md (deflated 65%)\n",
      "  adding: models/lora-adapter/checkpoint-123/adapter_model.safetensors (deflated 7%)\n",
      "  adding: models/lora-adapter/checkpoint-123/optimizer.pt (deflated 11%)\n",
      "  adding: models/lora-adapter/checkpoint-123/adapter_config.json (deflated 56%)\n",
      "  adding: models/lora-adapter/checkpoint-123/trainer_state.json (deflated 73%)\n",
      "  adding: models/lora-adapter/checkpoint-123/scaler.pt (deflated 64%)\n",
      "  adding: models/lora-adapter/checkpoint-123/scheduler.pt (deflated 61%)\n",
      "  adding: models/lora-adapter/README.md (deflated 65%)\n",
      "  adding: models/lora-adapter/adapter_model.safetensors (deflated 7%)\n",
      "  adding: models/lora-adapter/tokenizer_config.json (deflated 96%)\n",
      "  adding: models/lora-adapter/special_tokens_map.json (deflated 63%)\n",
      "  adding: models/lora-adapter/checkpoint-82/ (stored 0%)\n",
      "  adding: models/lora-adapter/checkpoint-82/rng_state.pth (deflated 26%)\n",
      "  adding: models/lora-adapter/checkpoint-82/training_args.bin (deflated 53%)\n",
      "  adding: models/lora-adapter/checkpoint-82/README.md (deflated 65%)\n",
      "  adding: models/lora-adapter/checkpoint-82/adapter_model.safetensors (deflated 7%)\n",
      "  adding: models/lora-adapter/checkpoint-82/optimizer.pt (deflated 11%)\n",
      "  adding: models/lora-adapter/checkpoint-82/adapter_config.json (deflated 56%)\n",
      "  adding: models/lora-adapter/checkpoint-82/trainer_state.json (deflated 69%)\n",
      "  adding: models/lora-adapter/checkpoint-82/scaler.pt (deflated 64%)\n",
      "  adding: models/lora-adapter/checkpoint-82/scheduler.pt (deflated 61%)\n",
      "  adding: models/lora-adapter/chat_template.jinja (deflated 72%)\n",
      "  adding: models/lora-adapter/adapter_config.json (deflated 56%)\n",
      "  adding: models/lora-adapter/checkpoint-41/ (stored 0%)\n",
      "  adding: models/lora-adapter/checkpoint-41/rng_state.pth (deflated 26%)\n",
      "  adding: models/lora-adapter/checkpoint-41/training_args.bin (deflated 53%)\n",
      "  adding: models/lora-adapter/checkpoint-41/README.md (deflated 65%)\n",
      "  adding: models/lora-adapter/checkpoint-41/adapter_model.safetensors (deflated 7%)\n",
      "  adding: models/lora-adapter/checkpoint-41/optimizer.pt (deflated 11%)\n",
      "  adding: models/lora-adapter/checkpoint-41/adapter_config.json (deflated 56%)\n",
      "  adding: models/lora-adapter/checkpoint-41/trainer_state.json (deflated 63%)\n",
      "  adding: models/lora-adapter/checkpoint-41/scaler.pt (deflated 64%)\n",
      "  adding: models/lora-adapter/checkpoint-41/scheduler.pt (deflated 61%)\n",
      "\n",
      "✓ Model ready! Check local folder: models/lora-adapter/\n"
     ]
    }
   ],
   "source": [
    "# Zip and download\n",
    "!zip -r lora-adapter.zip models/lora-adapter/\n",
    "\n",
    "# For Colab extension, files should sync automatically\n",
    "# Check your local models/lora-adapter/ folder\n",
    "print(\"\\n✓ Model ready! Check local folder: models/lora-adapter/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Next: Deploy locally with `python serving/app.py`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
